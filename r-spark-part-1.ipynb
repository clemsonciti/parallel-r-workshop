{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to R and Spark for Big Data Analytics\n",
    "\n",
    "#### Spark in a nutshell\n",
    "\n",
    "- Spark is an implementation of the MapReduce programming paradigm that operates on in-memory data and allows data reuse across multiple computations. \n",
    "- Performance of Spark is significantly better than its predecessor, Hadoop MapReduce. \n",
    "- Spark's primary data abstraction is called a Resilient Distributed Dataset (RDD):\n",
    "  - Read-only, partitioned collection of records\n",
    "  - Created through deterministic operations on data (loading from stable storage or transforming from other RDDs)\n",
    "  - Do not need to be materialized at all times and are recoverable via data lineage\n",
    "\n",
    "<img src=\"figures/spark2_arch.png\" width=\"600\"/>\n",
    "\n",
    "#### Spark and R\n",
    "\n",
    "- Spark's ability to inherently leverage and combine CPU and memory across computing nodes address R's memory limitation and sequential processing issues. \n",
    "- There exists various bindings between R and Spark. \n",
    "- Package **sparklyr** is an open source solution from RStudio's creators. This package offers the following advantages in combining Spark and R:\n",
    "  - Complete **dplyr** backend to support transparent data manipulation on Spark cluster from inside R. \n",
    "  - Implementation to support Spark's distributed machine learning library from R. \n",
    "  - Extensions that allow users to call the full Spark API from inside R and access other custom Spark packages. \n",
    "\n",
    "\n",
    "#### SparkR (older, not as powerful as sparklyr)\n",
    "\n",
    "- \"SparkR is an R package that provides a frontend to Apache Spark and uses Spark’s distributed computation engine to enable large scale data analysis from the R shell\"\n",
    "- \"The central component of SparkR is a distributed data frame implemented on top of Spark. SparkR DataFrames have an API similar to dplyr or local R data frames, but scale to large datasets using Spark’s execution engine and relational query optimizer\".\n",
    "- *Venkataraman, Shivaram, et al. \"Sparkr: Scaling r programs with spark.\" Proceedings of the 2016 International Conference on Management of Data. ACM, 2016.*\n",
    "\n",
    "<img src=\"figures/sparkr.jpg\" width=\"600\"/>\n",
    "\n",
    "\n",
    "#### Learning Objectives\n",
    "\n",
    "- Know how to deploy a Spark environment inside R using the **sparklyr** package\n",
    "- Know how to manipulate data inside Spark from R\n",
    "  - Understand the differences between data on the local R environment versus data inside the Spark environment\n",
    "- Know how to utilize analytic functions from **sparklyr** package\n",
    "- Know how to write and execute R codes inside the Spark environment\n",
    "- Know how to access Spark API from inside R\n",
    "\n",
    "#### Materials on this notebook is based on the following resources:\n",
    "\n",
    "- [sparklyr: R interface for Apache Spark](http://spark.rstudio.com)\n",
    "- [Airline Data Set](http://stat-computing.org/dataexpo/2009/the-data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark environment on Palmetto\n",
    "\n",
    "With the **hdp/0.1** module, Palmetto users can have full access to Cypress, Clemson's Hadoop Big Data infrastructure, from any computing node on Palmetto. To ensure that this module is available to R, run the followings:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where am I?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "getwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear global environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rm(list = ls())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup *sparklyr*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setupLibrary <- function(libraryName){\n",
    "  if (!require(libraryName, character.only = TRUE)){\n",
    "    install.packages(libraryName, dep = TRUE)\n",
    "    if (!require(libraryName, character.only = TRUE)){\n",
    "      print('Package not found')\n",
    "    }\n",
    "  } else {\n",
    "    print('Package is loaded')\n",
    "  }\n",
    "}\n",
    "\n",
    "setupLibrary('sparklyr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "system('spark-submit --help')\n",
    "y <- system2('printenv', args = c('| grep SPARK'), stdout = TRUE)\n",
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc <- spark_connect(master = 'yarn', \n",
    "                    config = list('spark.driver.memory'='8G',\n",
    "                                  'spark.executor.instances'=4,\n",
    "                                  'spark.executor.cores'=8,\n",
    "                                  'spark.executor.memory'='8G')\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chunk will spawn a Spark cluster inside Cypress that has a total of 32 computing cores and 32 GB of memory. The Spark driver accessiable from R's local environment has 8GB memory. \n",
    "\n",
    "To customize the Spark environment configurations, you can combine *sparklyr.shell.* with the corresponding options from *spark-submit*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "system2('spark-submit',args = c('--help'), stderr = TRUE, stdout = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that our Spark cluster is up and running, we send a command to Cypress via R's `system2` function. The original command is:\n",
    "\n",
    "`yarn application -list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "system2('yarn',args = c('application','-list'), stderr = TRUE, stdout = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the status and resource usage of the running application with the following command:\n",
    "\n",
    "`yarn application -status <applicationId>`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "system2('yarn',args = c('application','-status','application_1511291493821_0711'), stderr = TRUE, stdout = TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "system2('hdfs',args = c('dfs','-ls','-h','/repository/movielens'), stderr = TRUE, stdout = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_tlb <- spark_read_csv(sc, name = 'airline_data',\n",
    "                            path = '/repository/movielens/ratings.csv',\n",
    "                            delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where is the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim(movie_tlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdf_dim(movie_tlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "object.size(movie_tlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "movie_tlb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify movies with more than 10,000 reviews and average ratings that are higher than 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "setupLibrary('dplyr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_ratings <- movie_tlb %>% \n",
    "  group_by(movieId) %>%\n",
    "  summarise(count = n(), avg_rating = mean(rating)) %>%\n",
    "  filter(count >= 10000, avg_rating >= 4.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdf_dim(avg_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment rating data with movie information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info_tlb <- spark_read_csv(sc, name = 'movie_data',\n",
    "                            path = '/repository/movielens/movies.csv',\n",
    "                            delimiter = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "info_tlb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sdf_dim(info_tlb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_movies <- avg_ratings %>%\n",
    "  inner_join(info_tlb, by = 'movieId') %>%\n",
    "  select(title, count, avg_rating) %>%\n",
    "  collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark_disconnect(sc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R 3.4.2",
   "language": "R",
   "name": "ir342"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
